import numpy as np
from dataset.mnist import load_mnist
from multi_layer_net import MultiLayerNetExtend
from common.optimizer import Momentum
import matplotlib.pyplot as plt

# 데이터 로드
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

# 하이퍼파라미터 설정
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rates = np.arange(0.001, 0.02, 0.005)  # 학습률 범위 설정
hidden_sizes = np.arange(5, 16)  # 은닉층 크기 범위 설정
momentum = 0.9  # 모멘텀 파라미터 설정

train_loss_list = []  # 손실 함수 값 기록을 위한 리스트 추가
train_acc_list = []
test_acc_list = []
lr_hs_combinations = []  # 각 조합별 학습률과 은닉층 크기 기록을 위한 리스트 추가

# 하이퍼파라미터 튜닝
for lr in learning_rates:
    for hidden_size in hidden_sizes:
        network = MultiLayerNetExtend(input_size=784, hidden_size_list=[hidden_size, hidden_size, hidden_size], output_size=10,
                                      activation='relu', weight_init_std='relu', use_batchnorm=True)
        optimizer = Momentum(lr, momentum)

        train_loss = []  # 각 조합별 학습 손실을 기록하기 위한 리스트 추가
        train_acc = []
        test_acc = []

        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            t_batch = t_train[batch_mask]

            grads = network.gradient(x_batch, t_batch)
            optimizer.update(network.params, grads)

            loss = network.loss(x_batch, t_batch)
            train_loss.append(loss)

            if i % 100 == 0:
                train_acc_val = network.accuracy(x_train, t_train)
                test_acc_val = network.accuracy(x_test, t_test)
                train_acc.append(train_acc_val)
                test_acc.append(test_acc_val)
                print(f"Iteration: {i}, Train Acc: {train_acc_val}, Test Acc: {test_acc_val}")
                
                if train_acc_val >= 0.95 and test_acc_val >= 0.95:
                    train_loss_list.append(train_loss)
                    train_acc_list.append(train_acc)
                    test_acc_list.append(test_acc)
                    lr_hs_combinations.append((lr, hidden_size))
                    print(f"Learning Rate: {lr}, Hidden Size: {hidden_size}")
                    break

plt.figure(figsize=(10, 5))

# 학습 정확도 그래프
plt.subplot(1, 2, 1)
for i, train_acc in enumerate(train_acc_list):
    iterations = range(len(train_acc))
    lr, hs = lr_hs_combinations[i]
    plt.plot(iterations, train_acc, label=f"LR: {lr}, Hidden: {hs}")
plt.title('학습 정확도 진행')  # 제목 수정
plt.xlabel('반복(Iteration)')
plt.ylabel('정확도')
plt.ylim(0.0, 1.0)
plt.legend()

# 테스트 정확도 그래프
plt.subplot(1, 2, 2)
for i, test_acc in enumerate(test_acc_list):
    iterations = range(len(test_acc))
    lr, hs = lr_hs_combinations[i]
    plt.plot(iterations, test_acc, label=f"LR: {lr}, Hidden: {hs}")
plt.title('테스트 정확도 진행')  # 제목 수정
plt.xlabel('반복(Iteration)')
plt.ylabel('정확도')
plt.ylim(0.0, 1.0)
plt.legend()

plt.show()
