import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from multi_layer_net import MultiLayerNetExtend
from common.optimizer import Momentum
from common.util import smooth_curve

# 데이터 로드
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

# 하이퍼파라미터 설정
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rates = np.arange(0.001, 0.02, 0.008)  # 학습률 범위 설정
hidden_sizes = np.arange(5, 16)  # 은닉층 크기 범위 설정
momentum = 0.9  # 모멘텀 파라미터 설정

train_loss_list = []  # 손실 함수 값 기록을 위한 리스트 추가
train_acc_list = []
test_acc_list = []
lr_hs_combinations = []  # 각 조합별 학습률과 은닉층 크기 기록을 위한 리스트 추가

# 하이퍼파라미터 튜닝
for lr in learning_rates:
    for hidden_size in hidden_sizes:
        network = MultiLayerNetExtend(input_size=784, hidden_size_list=[hidden_size, hidden_size, hidden_size], output_size=10,
                                      activation='relu', weight_init_std='relu', use_batchnorm=True)
        optimizer = Momentum(lr, momentum)

        train_loss = []  # 각 조합별 학습 손실을 기록하기 위한 리스트 추가
        train_acc = []
        test_acc = []

        for i in range(iters_num):
            batch_mask = np.random.choice(train_size, batch_size)
            x_batch = x_train[batch_mask]
            t_batch = t_train[batch_mask]

            grads = network.gradient(x_batch, t_batch)
            optimizer.update(network.params, grads)

            loss = network.loss(x_batch, t_batch)
            train_loss.append(loss)

            if i % 100 == 0:
                train_acc_val = network.accuracy(x_train, t_train)
                test_acc_val = network.accuracy(x_test, t_test)
                train_acc.append(train_acc_val)
                test_acc.append(test_acc_val)
                print(f"Iteration: {i}, Train Acc: {train_acc_val}, Test Acc: {test_acc_val}")
                
                if train_acc_val >= 0.95 and test_acc_val >= 0.95:
                    train_loss_list.append(train_loss)
                    train_acc_list.append(train_acc)
                    test_acc_list.append(test_acc)
                    lr_hs_combinations.append((lr, hidden_size))
                    print(f"Learning Rate: {lr}, Hidden Size: {hidden_size}")
                    break

# 그래프 그리기
markers = {"Train Acc": "o", "Test Acc": "x"}
plt.figure(figsize=(10, 5))

# 학습 및 테스트 정확도 그래프
for i, (train_acc, test_acc) in enumerate(zip(train_acc_list, test_acc_list)):
    iterations = np.arange(len(train_acc)) * 100  # 100 iteration 마다 기록된 값
    lr, hs = lr_hs_combinations[i]
    plt.plot(iterations, smooth_curve(train_acc), marker=markers["Train Acc"], markevery=2, label=f"LR: {lr}, Hidden: {hs}, Train")
    plt.plot(iterations, smooth_curve(test_acc), marker=markers["Test Acc"], markevery=2, label=f"LR: {lr}, Hidden: {hs}, Test")
plt.xlabel("Iteration")
plt.ylabel("Accuracy")
plt.ylim(0.0, 1.0)
plt.title('Training Accuracy')
plt.legend()
plt.show()
